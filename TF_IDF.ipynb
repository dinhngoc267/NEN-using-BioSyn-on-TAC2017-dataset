{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6TM34S7v8CbvqGHYRHyrX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinhngoc267/NSEEN/blob/master/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2pdHrC3Ej0z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1772e518-d1af-4b99-f71a-ec2409251e17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA9aYI_QEtAa"
      },
      "source": [
        "import keras\n",
        "\n",
        "import os \n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf \n",
        "import tensorflow_hub as hub \n",
        "import tensorflow_datasets as tfds \n",
        "\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import copy\n",
        "\n",
        "import xml.etree.ElementTree as elemTree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_IGVzUoE4CF"
      },
      "source": [
        "# Define func to encode raw_text to token\n",
        "# vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "# do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "# tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n",
        "\n",
        "def bert_encode(texts, tokenizer, max_len=25):\n",
        "    all_tokens = []\n",
        "    all_masks = []\n",
        "    all_segments = []\n",
        "    \n",
        "    for text in texts:\n",
        "        text = tokenizer.tokenize(text)\n",
        "        \n",
        "        text = text[:max_len-2]\n",
        "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
        "        pad_len = max_len - len(input_sequence)\n",
        "        \n",
        "        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0]*pad_len\n",
        "        pad_masks = [1] * len(input_sequence) + [0]*pad_len\n",
        "        segment_ids = [0]*max_len\n",
        "        \n",
        "        all_tokens.append(tokens)\n",
        "        all_masks.append(pad_masks)\n",
        "        all_segments.append(segment_ids)\n",
        "        \n",
        "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
        "\n",
        "class DenseEncoder():\n",
        "  def __init__(self, bert_layer, max_len=25):\n",
        "    \n",
        "    input_word_ids = tf.keras.Input(shape=(max_len,),dtype=tf.int32, name=\"input_word_ids\")\n",
        "    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
        "    segment_ids = tf.keras.Input(shape=(max_len,),dtype=tf.int32, name=\"segment_ids\")\n",
        "\n",
        "    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask,segment_ids])\n",
        "    clf_output = sequence_output[:,0,:]\n",
        "\n",
        "    self.model = tf.keras.models.Model(inputs=[input_word_ids,input_mask,segment_ids], outputs=clf_output)\n",
        "\n",
        "  def get_model(self):\n",
        "    return self.model\n",
        "\n",
        "  def get_dense_embedding(self,input):\n",
        "    return self.model.predict([input])\n",
        "\n",
        "class SparseEncoder(object):\n",
        "  def __init__(self):\n",
        "    self.encoder = TfidfVectorizer(analyzer='char', ngram_range=(1,2))\n",
        "\n",
        "  def fit(self, train_corpus):\n",
        "    self.encoder.fit(train_corpus)\n",
        "\n",
        "    return self\n",
        "  \n",
        "  def transform(self, mentions):\n",
        "    vec = self.encoder.transform(mentions).toarray()\n",
        "    #vec = tf.constant(vec, dtype=tf.float32)\n",
        "    return vec\n",
        "  \n",
        "  def __call__(self, mentions):\n",
        "    return self.transform(mentions)\n",
        "\n",
        "\n",
        "class QueryDataset():\n",
        "  def __init__(self, data_dir):\n",
        "    self.data = self.load_data(data_dir = data_dir)\n",
        "\n",
        "  def load_data(self, data_dir):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    data_dir: a path of data\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data: np.array[(mention, CUI)]\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    concept_files = glob.glob(os.path.join(data_dir,\"*.concept\"))\n",
        "    for concept_file in tqdm(concept_files):\n",
        "      with open(concept_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        concepts = f.readlines()\n",
        "\n",
        "        for concept in concepts: \n",
        "          concept = concept.split(\"||\")\n",
        "          mention = concept[3].strip()\n",
        "          cui = concept[4].strip()\n",
        "\n",
        "          data.append((mention,cui))\n",
        "\n",
        "    data = np.array(data)\n",
        "\n",
        "    return data\n",
        "\n",
        "class DictionaryDataset():\n",
        "  \"\"\"\n",
        "  Dictionary data\n",
        "  \"\"\"\n",
        "  def __init__(self, dictionary_path):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    dictionary_path: str\n",
        "      The path of the dictionary\n",
        "    \"\"\"\n",
        "    self.data = self.load_data(dictionary_path)\n",
        "  def load_data(self, dictionary_path):\n",
        "    data = []\n",
        "    with open(dictionary_path, mode='r', encoding='utf-8') as f:\n",
        "      lines = f.readlines()\n",
        "      for line in tqdm(lines):\n",
        "        line = line.strip()\n",
        "        if line==\"\": continue\n",
        "        cui, name = line.split(\"||\")\n",
        "        data.append((name,cui))\n",
        "\n",
        "    data = np.array(data)\n",
        "    return data\n",
        "\n",
        "class Scalar(Layer):\n",
        "  def __init__(self,name=None):\n",
        "    super(Scalar, self).__init__(name=name)\n",
        "  def build(self,input_shape) :\n",
        "    self.W = K.variable(0)\n",
        "    self._trainable_weights=[self.W]\n",
        "    super().build(input_shape)\n",
        "  def call(self,inputs):\n",
        "    return self.W*inputs\n",
        "\n",
        "class MedDRAPreprocess():\n",
        "  \"\"\"\n",
        "  Make training dictionary pair\n",
        "  \"\"\"\n",
        "  def __init__(self, hlgt_path, hlt_path, pt_path, llt_path):\n",
        "      self.hlgt_path = hlgt_path\n",
        "      self.hlt_path = hlt_path\n",
        "      self.pt_path =pt_path\n",
        "      self.llt_path = llt_path\n",
        "\n",
        "  def load_dictionary(self):\n",
        "      \"\"\" \n",
        "      ! hlgt, hlt, pt => need to extract id and name\n",
        "      format id$name$$$$$$$$\n",
        "      ! llt => need to extract pt_id and name\n",
        "      format id$name$pt_id$$$$$$$\n",
        "      \"\"\"\n",
        "\n",
        "      dictionary = {}\n",
        "      # hlgt\n",
        "      with open(self.hlgt_path, \"r\") as f:\n",
        "          lines = f.readlines()\n",
        "          for line in tqdm(lines):\n",
        "              line = line.split(\"$\")\n",
        "              _id = line[0]\n",
        "              _name = line[1]\n",
        "              if _id not in dictionary.keys():\n",
        "                  dictionary[_id] = _name  \n",
        "              else:\n",
        "                  dictionary[_id] = dictionary[_id] + \"|\" + _name\n",
        "      # hlt\n",
        "      with open(self.hlt_path, \"r\") as f:\n",
        "          lines = f.readlines()\n",
        "          for line in tqdm(lines):\n",
        "              line = line.split(\"$\")\n",
        "              _id = line[0]\n",
        "              _name = line[1]\n",
        "              if _id not in dictionary.keys():\n",
        "                  dictionary[_id] = _name  \n",
        "              else:\n",
        "                  dictionary[_id] = dictionary[_id] + \"|\" + _name\n",
        "\n",
        "      # pt\n",
        "      with open(self.pt_path, \"r\") as f:\n",
        "          lines = f.readlines()\n",
        "          for line in tqdm(lines):\n",
        "              line = line.split(\"$\")\n",
        "              _id = line[0]\n",
        "              _name = line[1]\n",
        "              if _id not in dictionary.keys():\n",
        "                  dictionary[_id] = _name  \n",
        "              else:\n",
        "                  dictionary[_id] = dictionary[_id] + \"|\" + _name\n",
        "\n",
        "      # llt\n",
        "      with open(self.llt_path, \"r\") as f:\n",
        "          lines = f.readlines()\n",
        "          for line in tqdm(lines):\n",
        "              line = line.split(\"$\")\n",
        "              _id = line[2]\n",
        "              _name = line[1]\n",
        "              if _id not in dictionary.keys():\n",
        "                  dictionary[_id] = _name  \n",
        "              else:\n",
        "                  names = dictionary[_id].split(\"|\")\n",
        "                  names.append(_name)\n",
        "                  names = \"|\".join(list(set(names)))\n",
        "                  dictionary[_id] = names\n",
        "\n",
        "      list_dictionary = [[k,v] for k,v in dictionary.items()]\n",
        "      return list_dictionary\n",
        "\n",
        "  def make_ID_mention_map(self, out):\n",
        "      dictionary = self.load_dictionary()\n",
        "      with open(out, 'w') as outfile:\n",
        "          for row in dictionary:\n",
        "              outfile.write(\"||\".join(row))\n",
        "              outfile.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WlxiatJEzEt"
      },
      "source": [
        "def parse_xml(file):\n",
        "    doc = elemTree.parse(file)\n",
        "    root = doc.getroot()\n",
        "\n",
        "    # text\n",
        "    text = \"\"\n",
        "    sections = root.findall('./Text/Section')\n",
        "    for section in sections:\n",
        "        text += section.text\n",
        "\n",
        "    # reaction nodes have all mention and id.\n",
        "    mention2id = {}\n",
        "    reactions = root.findall('./Reactions/Reaction')\n",
        "    for reaction in reactions:\n",
        "        mention = reaction.attrib['str'].lower()\n",
        "        cuis = []\n",
        "        for normalization in reaction.findall('Normalization'):\n",
        "            if 'meddra_pt_id' in normalization.attrib:\n",
        "                cuis.append(normalization.attrib['meddra_pt_id'])\n",
        "        cuis = '|'.join(cuis)\n",
        "        if mention not in mention2id:\n",
        "            mention2id[mention] = cuis\n",
        "        else:\n",
        "            raise ValueError('mention({}) already have id({}) in mention2id dictionary.'.format(mention, id_))\n",
        "    \n",
        "    # mention node have section and span information.\n",
        "    entity_mentions = [mention for mention in mention2id.keys()]\n",
        "    entity_ids = [mention2id[mention] if mention2id[mention] else '-1' for mention in entity_mentions]\n",
        "    id_name = list(zip(entity_ids, entity_mentions))\n",
        "\n",
        "    return text, id_name\n",
        "\n",
        "def process_TAC(input_dir, output_dir):\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    \n",
        "    input_files = sorted(glob.glob(os.path.join(input_dir, \"*.xml\")))\n",
        "    num_docs=0\n",
        "    num_queries = 0\n",
        "    for input_file in input_files:\n",
        "        document_name = os.path.basename(input_file).split(\".\")[0]\n",
        "        txtname = document_name + '.txt'\n",
        "        conceptname = document_name + '.concept'\n",
        "\n",
        "        text, id_names = parse_xml(input_file)\n",
        "\n",
        "        # save text\n",
        "        with open(os.path.join(output_dir,txtname) ,'w') as f:\n",
        "            f.write(text.lower())\n",
        "        \n",
        "        # save entity\n",
        "        with open(os.path.join(output_dir,conceptname) ,'w') as f:\n",
        "            for cui, mention in id_names:\n",
        "                f.write(\"-1||-1|-1||-1||{}||{}\".format(mention, cui))\n",
        "                f.write(\"\\n\")\n",
        "                num_queries +=1\n",
        "        num_docs+=1\n",
        "        \n",
        "    print(\"{} {} {}\".format(output_dir, num_docs,num_queries))    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DMchYO6puE-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b462acd-94bb-49ec-ae3b-455af99362f2"
      },
      "source": [
        "# load data\n",
        "training_query = QueryDataset('/content/drive/My Drive/TAC2017/train')\n",
        "training_dictionary = DictionaryDataset('/content/drive/My Drive/MedDRA/dictionary/meddra18.1.txt')\n",
        "test_query = QueryDataset('/content/drive/My Drive/TAC2017/test')\n",
        "test_dictionary = DictionaryDataset('/content/drive/My Drive/MedDRA/dictionary/meddra18.1.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 80/80 [00:21<00:00,  3.68it/s]\n",
            "100%|██████████| 23668/23668 [00:00<00:00, 511251.81it/s]\n",
            "100%|██████████| 99/99 [00:27<00:00,  3.60it/s]\n",
            "100%|██████████| 23668/23668 [00:00<00:00, 770060.33it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjiOEFvwiylm"
      },
      "source": [
        "def find_min_idx(array):\n",
        "  \"\"\"\n",
        "  Returns: the index of min in array\n",
        "  \"\"\"\n",
        "  min_idx = 0\n",
        "\n",
        "  for idx, item in enumerate(array):\n",
        "    if item < array[min_idx]:\n",
        "      min_idx = idx\n",
        "\n",
        "  return min_idx \n",
        "\n",
        "def find_topk_candidate(array, topk):\n",
        "  topk_idx = []\n",
        "\n",
        "  for idx,item in enumerate(array):\n",
        "    if len(topk_idx) < topk:\n",
        "      topk_idx.append(idx)\n",
        "    else:\n",
        "      min_idx = find_min_idx(array[topk_idx])\n",
        "      if item > array[topk_idx[min_idx]]:\n",
        "        topk_idx[min_idx] = idx\n",
        "  return np.array(topk_idx)\n",
        "\n",
        "def sort_candidate_descening(array_value, array_idx):\n",
        "\n",
        "  for i in range(len(array_value)-1):\n",
        "    max_idx = i\n",
        "    for j in range(i+1, len(array_value)):\n",
        "      if array_value[j] > array_value[max_idx]:\n",
        "        max_idx = j\n",
        "    if max_idx != i:\n",
        "      \n",
        "      tmp = array_value[i]\n",
        "      array_value[i] = array_value[max_idx]\n",
        "      array_value[max_idx] = tmp\n",
        "\n",
        "      tmp = array_idx[i]\n",
        "      array_idx[i] = array_idx[max_idx]\n",
        "      array_idx[max_idx] = tmp\n",
        "  return array_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07NvDvfT1ogC"
      },
      "source": [
        "sparse_encoder = SparseEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqrLF6ChuOLd"
      },
      "source": [
        "# define function to get sparse candidates and dense candidates indices of query\n",
        "\n",
        "def get_sparse_candidate_indices(query_text, corpus, sparse_encoder, topk=20):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    query_text: list of query string\n",
        "    corpus: list of text in dictionary\n",
        "    sparse_encoder: sparse encoder which embeds text to vector base tf-idf method\n",
        "  \n",
        "  Returns: \n",
        "    np.darray: matrix of indices of candidate in dictionary base on sparse score. Shape: (len(query), topk)\n",
        "  \"\"\"\n",
        "  sparse_encoder.fit(corpus)\n",
        "  query_sparse_embeddings = sparse_encoder.transform(query_text)\n",
        "  dictionary_sparse_embeddings = sparse_encoder.transform(corpus)\n",
        "\n",
        "  score_sparse_matrix = np.dot(query_sparse_embeddings, dictionary_sparse_embeddings.transpose())\n",
        "  \n",
        "  sparse_candidates = []\n",
        "  for i in range(0, len(score_sparse_matrix)):\n",
        "    topk_candidates_sparse =  np.argpartition(score_sparse_matrix[i], -topk)[-topk:]  # get n_sparse candidate first\n",
        "    array_value = copy.deepcopy(score_sparse_matrix[i][topk_candidates_sparse])\n",
        "    topk_candidates_sparse = sort_candidate_descening(array_value, topk_candidates_sparse)\n",
        "    sparse_candidates.append(topk_candidates_sparse)\n",
        "\n",
        "  sparse_candidates = np.array(sparse_candidates)\n",
        "  return sparse_candidates\n",
        "\n",
        "def get_dense_candidate_indices(query_text, corpus, bert_layer,topk=20):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    query_text: list of query string\n",
        "    corpus: list of text in dictionary\n",
        "    bert_layer: bert-pre-trained\n",
        "  \n",
        "  Returns: \n",
        "    np.darray: matrix of indices of candidate in dictionary base on dense score. Shape: (len(query), topk)\n",
        "  \"\"\"\n",
        "  query_tokens = bert_encode(query_text, tokenizer, max_len=25)\n",
        "  dictionary_tokens = bert_encode(corpus, tokenizer, max_len=25)\n",
        "  dense_encoder = DenseEncoder(bert_layer, max_len=25)\n",
        "  query_dense_embeddings = dense_encoder.get_dense_embedding(query_tokens) # [None, 768]\n",
        "  dictionary_dense_embeddings =dense_encoder.get_dense_embedding(dictionary_tokens) # [None, 768]\n",
        "  \n",
        "  query_dense_score = np.dot(query_dense_embeddings, tf.transpose(dictionary_dense_embeddings, perm=[1,0]))\n",
        "  \n",
        "  candidates_dense = []\n",
        "  for i in range(0, len(query_dense_score)):\n",
        "    topk_candidates_dense = np.argpartition(query_dense_score[i], -topk)[-topk:]  # get n_sparse candidate first\n",
        "    array_value = copy.deepcopy(query_dense_score[i][topk_candidates_dense])\n",
        "    topk_candidates_dense = sort_candidate_descening(array_value, topk_candidates_dense)\n",
        "    candidates_dense.append(topk_candidates_dense)\n",
        "\n",
        "  candidates_dense = np.array(candidates_dense)\n",
        "  return candidates_dense\n",
        "\n",
        "def get_query_candidates_indices(query_text, corpus, sparse_encoder, bert_layer, topk=20):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    candidate_dense_indices: matrix of dense candidate indices\n",
        "    candidate_sparse_indices: matrix of sparse candidate indices\n",
        "    topk: number of candidates of a query\n",
        "  \n",
        "  Returns: \n",
        "    np.darray: matrix of indices of candidate in dictionary base on dense and sparse score. Shape: (len(query), topk)\n",
        "  \"\"\"\n",
        "\n",
        "  sparse_candidate_indices = get_sparse_candidate_indices(query_text, corpus, sparse_encoder,topk)\n",
        "  dense_candidate_indices = get_dense_candidate_indices(query_text, corpus, bert_layer, topk=30)\n",
        "\n",
        "  candidates_indices = np.empty((sparse_candidate_indices.shape[0],topk))\n",
        "  candidates_indices[:,0:11] = sparse_candidate_indices[:,0:11]\n",
        "  for idx, row in enumerate(candidates_indices):\n",
        "    n = 10\n",
        "    for i in range(topk):\n",
        "      if dense_candidate_indices[idx][i] not in row:\n",
        "        row[n] = dense_candidate_indices[idx][i] \n",
        "        n += 1;      \n",
        "      if n == topk: \n",
        "        break;\n",
        "    \n",
        "  return candidates_indices\n",
        "\n",
        "def get_dense_candidate_embeddings(candidate_indices, dictionary, bert_layer):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    candidate_indices: matrix of candidate indices\n",
        "    dictionary: dictionary\n",
        "    bert_layer: bert-pretrained packaged in layer\n",
        "  \n",
        "  Returns: \n",
        "    np.darray: matrix of dense embeddings of candidates of queries. Shape: (len(query), topk, 768)\n",
        "  \"\"\"\n",
        "\n",
        "  candidates_raw_text = []\n",
        "  for row in candidate_indices:\n",
        "    row_texts = dictionary.data[row.astype(int).tolist(),0]\n",
        "    candidates_raw_text.append(row_texts)\n",
        "\n",
        "  candidates_raw_text = np.array(candidates_raw_text)\n",
        "  candidate_tokens = []\n",
        "  for row in candidates_raw_text:\n",
        "    row_tokens = bert_encode(row, tokenizer,max_len=25)\n",
        "    candidate_tokens.append(row_tokens)\n",
        "  \n",
        "  dense_encoder = DenseEncoder(bert_layer, max_len=25)\n",
        "  candidate_embeddings = []\n",
        "  for row in candidate_tokens:\n",
        "    row_embeddings = dense_encoder.get_dense_embedding(row)\n",
        "    candidate_embeddings.append(row_embeddings)\n",
        "\n",
        "  candidate_embeddings = np.array(candidate_embeddings)\n",
        "  return candidate_embeddings\n",
        "\n",
        "#candidate_embeddings = get_query_candidates_embeddings(candidates_indices, training_dictionary, bert_layer)\n",
        "def get_sparse_candidate_score(candidates_indices, sparse_score_matix):\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    candidate_indices: matrix of candidate indices\n",
        "    sparse_score_matix: sparse score of all queries\n",
        "    bert_layer: bert-pretrained packaged in layer\n",
        "  \n",
        "  Returns: \n",
        "    np.darray: matrix of dense embeddings of candidates of queries. Shape: (len(query), topk, 768)\n",
        "  \"\"\"\n",
        "\n",
        "  candidate_sparse_score = []\n",
        "  for idx,row in enumerate(candidates_indices):\n",
        "    row_sparse_score = sparse_score_matix[idx][row.astype(int).tolist()]\n",
        "    candidate_sparse_score.append(row_sparse_score)\n",
        "  \n",
        "  candidate_sparse_score = np.array(candidate_sparse_score)\n",
        "\n",
        "  return candidate_sparse_score\n",
        "\n",
        "def get_labels_of_candidates(true_labels, candidates_indices, dictionary):\n",
        "  label_candidates = []\n",
        "  for idx, row in enumerate(candidates_indices):\n",
        "\n",
        "    row_label = []\n",
        "    predict_label = dictionary.data[row.astype(int).tolist(),1]\n",
        "    for i, labels in enumerate(predict_label):\n",
        "      row_label.append(0)\n",
        "      labels = labels.split('|')\n",
        "      for label in labels:\n",
        "        if label in true_labels[idx]:\n",
        "          row_label[i] = 1\n",
        "    label_candidates.append(row_label)\n",
        "\n",
        "  return label_candidates\n",
        "\n",
        "def get_sparse_query_score(sparse_encoder, corpus, query_text):\n",
        "\n",
        "  sparse_encoder.fit(corpus)\n",
        "  query_sparse_embeddings = sparse_encoder.transform(query_text)\n",
        "  dictionary_sparse_embeddings = sparse_encoder.transform(corpus)\n",
        "  score_sparse_matrix = np.dot(query_sparse_embeddings, dictionary_sparse_embeddings.transpose())\n",
        "\n",
        "  return score_sparse_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB9edYwFizJa"
      },
      "source": [
        "# Define model ReRanker\n",
        "def marginal_loss(output, target):\n",
        "  predict = tf.nn.softmax(tf.cast(output, dtype=tf.float32))\n",
        "  loss = predict*target\n",
        "  loss = K.sum(loss,axis=-1)                  # sum all positive scores\n",
        "\n",
        "  loss = loss[loss > 0]                     # filter sets with at least one positives\n",
        "  #loss = K.clip(loss, min_value=1e-9, max_value=1) # for numerical stability\n",
        "  loss = -K.log(loss)                   # for negative log likelihood\n",
        "  if len(loss) == 0:\n",
        "      loss = K.sum(loss)                     # will return zero loss\n",
        "  else:\n",
        "      loss = K.mean(loss)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def build_model(bert_layer, topk=20, max_len=25):  \n",
        "  query_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"query_word_ids\")\n",
        "  query_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"query_mask\")\n",
        "  query_segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"query_segment_ids\")\n",
        "\n",
        "  pooled_output, sequence_output = bert_layer([query_word_ids, query_mask, query_segment_ids])\n",
        "  query_dense_embeddings = sequence_output[:, 0, :] # [None, 768]\n",
        "\n",
        "  # score: \n",
        "  candidates_dense_embeddings = tf.keras.Input(shape=(topk,768,), dtype=tf.float32, name=\"candidates_dense_embeddings\")\n",
        "  candidates_dense_score = keras.layers.Dot(axes=(2,1),name=\"dense_score\")([candidates_dense_embeddings,query_dense_embeddings])\n",
        "  \n",
        "  batch_size = candidates_dense_score.shape[0] \n",
        "  candidate_sparse_score = tf.keras.Input(shape=(batch_size,), dtype=tf.float32, name=\"candidate_sparse_score\")\n",
        "  scaling_sparse_score = Scalar(name='sparse_weight')(candidate_sparse_score)\n",
        "\n",
        "  score = scaling_sparse_score + candidates_dense_score\n",
        "\n",
        "  lr_multiplier = {\n",
        "    'bert_layer':1, # optimize with a smaller learning rate\n",
        "    'sparse_weight':0.5e+3   # optimize  with a larger learning rate\n",
        "    }\n",
        "    \n",
        "  opt = AdamLRM(lr=1e-5, lr_multiplier=lr_multiplier)\n",
        "\n",
        "  model =  tf.keras.models.Model(inputs = [[query_word_ids, query_mask, query_segment_ids],candidates_dense_embeddings, candidate_sparse_score], outputs=score )\n",
        "  #model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-5),loss=marginal_loss)\n",
        "  model.compile(optimizer=opt,loss=marginal_loss)\n",
        "  return model  \n",
        "\n",
        "def retreival(query, dictionary,bert_layer, sparse_weight, max_len=25, topk=20):\n",
        "  sparse_encoder = SparseEncoder()\n",
        "  sparse_encoder.fit(dictionary.data[:,0])\n",
        "  query_sparse_embeddings = sparse_encoder.transform(query)\n",
        "  dictionary_sparse_embeddings = sparse_encoder.transform(dictionary.data[:,0])\n",
        "\n",
        "  query_sparse_score = np.dot(query_sparse_embeddings, dictionary_sparse_embeddings.transpose())\n",
        "  \n",
        "  query_tokens = bert_encode(query, tokenizer, max_len)\n",
        "  dictionary_tokens = bert_encode(dictionary.data[:,0], tokenizer, max_len)\n",
        "  dense_encoder = DenseEncoder(bert_layer, max_len)\n",
        "  query_dense_embeddings = dense_encoder.get_dense_embedding(query_tokens) \n",
        "  dictionary_dense_embeddings = dense_encoder.get_dense_embedding(dictionary_tokens) \n",
        "\n",
        "  query_dense_score = np.dot(query_dense_embeddings, tf.transpose(dictionary_dense_embeddings, perm=[1,0]))\n",
        "  \n",
        "  query_score = query_sparse_score*sparse_weight + query_dense_score\n",
        "\n",
        "  candidates_indices = []\n",
        "  for i in range(0, len(query_score)):\n",
        "    topk_candidates_dense = np.argpartition(query_score[i], -topk)[-topk:]  # get n_sparse candidate first\n",
        "    array_value = copy.deepcopy(query_score[i][topk_candidates_dense])\n",
        "    topk_candidates_dense = sort_candidate_descening(array_value, topk_candidates_dense)\n",
        "    candidates_indices.append(topk_candidates_dense)\n",
        "\n",
        "  candidates_indices = np.array(candidates_indices)\n",
        "  return candidates_indices\n",
        "\n",
        "def evaluate(true_labels,candidate_indices, dictionary):\n",
        "  get_labels_of_candidates\n",
        "  label_candidates = get_labels_of_candidates(true_labels,candidate_indices, dictionary)\n",
        "  n = len(label_candidates)\n",
        "  top_1 = top_5 = top_10 = top_20 = 0\n",
        "  for row in label_candidates:\n",
        "    if row[0] == 1:\n",
        "      top_1 += 1\n",
        "    if 1 in row[0:5]:\n",
        "      top_5 += 1\n",
        "    if 1 in row[0:10]:\n",
        "      top_10 += 1\n",
        "    if 1 in row[0:20]:\n",
        "      top_20 += 1\n",
        "    \n",
        "  return [top_1/n, top_5/n, top_10/n, top_20/n]\n",
        "#candidates_test_indices = retreival(training_query, training_dictionary, bert_layer, sparse_weight[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDcfUCY2yGWl"
      },
      "source": [
        "model = build_model(bert_layer,topk=20,max_len=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRjLxaxxSDXw"
      },
      "source": [
        "sparse_candidate_indices = get_sparse_candidate_indices(test_query.data[:,0],test_dictionary.data[:,0],sparse_encoder,topk=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXutab6nSxAD",
        "outputId": "543f071f-ffda-4943-9376-edbe0070962b"
      },
      "source": [
        "label_candidates = get_labels_of_candidates(test_query.data[:,1],sparse_candidate_indices, test_dictionary)\n",
        "label_candidates = np.array(label_candidates)\n",
        "n = len(label_candidates)\n",
        "top_1 = top_5 = top_10 = top_20 = 0\n",
        "for row in label_candidates:\n",
        "  if row[0] == 1:\n",
        "    top_1 += 1\n",
        "  if 1 in row[0:5]:\n",
        "    top_5 += 1\n",
        "  if 1 in row[0:10]:\n",
        "    top_10 += 1\n",
        "  if 1 in row[0:20]:\n",
        "    top_20 += 1\n",
        "\n",
        "#res = evaluate(training_query.data[:,1],sparse_candidate_indices, training_dictionary)\n",
        "#print('Top 1: {:.2f}% '.format(res[0]*100) + 'Top 5: {:.2f}% '.format(res[1]*100) + 'Top 10: {:.2f}% '.format(res[2]*100) + 'Top 20: {:.2f}% '.format(res[3]*100))\n",
        "print('Top 1: {:.2f}% '.format(top_1/n *100) + 'Top 5: {:.2f}% '.format(top_5/n *100) + 'Top 10: {:.2f}% '.format(top_10/n *100) + 'Top 20: {:.2f}% '.format(top_20/n *100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 1: 39.35% Top 5: 65.17% Top 10: 71.95% Top 20: 78.29% \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}